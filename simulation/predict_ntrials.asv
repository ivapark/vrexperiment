clear; close all; clc;


%% manage paths

[project_dir, ~]= fileparts(pwd);
[git_dir, ~] = fileparts(project_dir);
addpath(genpath(fullfile(git_dir, 'bads'))); % add optimization tool, here we use BADS for example
out_dir = fullfile(pwd, mfilename); % output will be saved to folder with the same name
if ~exist(out_dir, 'dir'); mkdir(out_dir); end

%% set up model

% Set covariance matrix. Extract real covariance matrix once data is available
sig_x = 0.1; % m, X and Y have similar smaller variances
sig_y = 0.1;
sig_z = 0.2; % Z has larger variance
sig_xy = 0;
sig_xz = 0.1;
sig_yz = 0.1;

Sigma = [sig_x, sig_xy, sig_xz;
    sig_xy, sig_y, sig_yz;
    sig_xz, sig_yz, sig_z];

% Adjust experiment parameters accordingly
model.sim_trial = 1e5; % number of simulations of reaches in optimization
model.empirical_cov = Sigma;
model.hit_threshold = 0.003; % m, target radius
model.hit_gain = 300;
model.penalty_gain = -500;
model.penalty_threshold = 0.003; % m, distance between target center and the edge of the penalty zone

% Used one coordinate, one penalty condition for example
model.target_coor = [-1, 1.1, 2.44];
model.penalty_cond = [-model.penalty_threshold, 0, 0];

%% set up model fitting

model.n_run = 7; % number of fits for each model
options.UncertaintyHandling = true;

%% Simulation start
%% 1. Predict optimal aiming point

curr_model =  str2func('optimal_gain');

model.mode = 'initialize';
val = curr_model([], model);
model.init_val = val;

model.mode = 'optimize';
neg_gain_func = @(x) -curr_model(x, model);

% For debug purpose
% testp = [0.13330078125 0 0.13330078125];
% test = neg_gain_func(testp);

% Fit the model multiple times with different initial values
est_p = nan(model.n_run, val.num_param);
neg_gain = nan(1, model.n_run);
parfor i  = 1:model.n_run
    temp_val = val;
    [est_p(i,:), neg_gain(i)] = bads(neg_gain_func,...
        temp_val.init(i,:), temp_val.lb, temp_val.ub, temp_val.plb, temp_val.pub);
end

% Find the best fits across runs
[min_neg_gain, best_idx] = min(neg_gain);
best_p = est_p(best_idx, :);
fits.param_info = val;
fits.est_p = est_p;
fits.best_p = best_p;
fits.max_gain = -min_neg_gain;

%% 2. Simulate sample mean difference between penalty and no penalty conditions

sim_ntrial = 1000; % number of simulation (i.e., participants)
p_ntrials = 10:5:20;
np_ntrials = 20:5:30;
opt_aim = model.target_coor + best_p;

CI_95 = nan(length(p_ntrials), length(np_ntrials), 2);
for mm = 1:length(p_ntrials)
    model.penalty_ntrial = p_ntrials(mm);

    for nn = 1:length(np_ntrials)
        model.no_penalty_ntrial = np_ntrials(nn);

        sample_mean_diff = nan(sim_ntrial, 1);

        for tt = 1:sim_ntrial
            M = model;

            % Simulate endpoints of penalty condition
            ep_penalty = mvnrnd(M.target_coor, M.empirical_cov, M.penalty_ntrial);

            % Simulate endpoints of no penalty condition
            ep_nopenalty = mvnrnd(opt_aim, M.empirical_cov, M.no_penalty_ntrial);

            % Calculate sample mean difference
            sample_mean_diff(tt) = mean(ep_penalty()) - mean(ep_nopenalty);
        end

        % Calculate 95% confidence interval
        CI_95(mm, nn, :) = prctile(sample_mean_diff, [2.5, 97.5]);
    end
end

%% 3. Plot results

figure;
tiledlayout(numel(p_ntrials), numel(np_ntrials)); % Penalty x no penalty

% Compute global x and y limits
all_vals = reshape(CI_95, [], 2);
xmin = min(all_vals(:));
xmax = max(all_vals(:));
edges = linspace(xmin, xmax, 100);

all_counts = [];
for mm = 1:numel(p_ntrials)
    for nn = 1:numel(np_ntrials)
        h_counts = histcounts(CI_95(mm, nn, :), edges);
        all_counts = [all_counts; h_counts];
    end
end
ymax = max(all_counts(:));

for mm = 1:numel(p_ntrials)
    for nn = 1:numel(np_ntrials)
        nexttile;
        histogram(CI_95(mm, nn, :), edges);
        xlim([xmin xmax]);
        ylim([0 ymax]);
        title({sprintf('#trials in penalty: %d', p_ntrials(mm)), sprintf('#trials in no penalty: %d', np_ntrials(nn))});
        xlabel('Sample mean difference');
        ylabel('Frequency');
    end
end


